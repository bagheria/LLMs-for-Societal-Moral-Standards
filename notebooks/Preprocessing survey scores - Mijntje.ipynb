{"cells":[{"cell_type":"markdown","metadata":{"id":"hj9KU_dtENqr"},"source":["# Preprocessing WVS & PEW and obtaining their moral scores\n"]},{"cell_type":"markdown","metadata":{"id":"UZxL08c2gItD"},"source":["### Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11380,"status":"ok","timestamp":1719530413242,"user":{"displayName":"Mijntje Meijer","userId":"17018801464945965112"},"user_tz":-120},"id":"OAATPNe8gULn","outputId":"5fd532cc-8d28-497b-f617-9b8acc88393f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyreadstat in /usr/local/lib/python3.10/dist-packages (1.2.7)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyreadstat) (2.0.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pyreadstat) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pyreadstat) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pyreadstat) (2024.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->pyreadstat) (1.25.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->pyreadstat) (1.16.0)\n"]}],"source":["!pip install pyreadstat"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6Ep5tDZLval"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","import transformers\n","\n","from tqdm import tqdm\n","from transformers import AutoTokenizer,AutoModelForCausalLM\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from scipy.stats import pearsonr"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ob6gZXgd6sNY","executionInfo":{"status":"ok","timestamp":1719530414834,"user_tz":-120,"elapsed":1599,"user":{"displayName":"Mijntje Meijer","userId":"17018801464945965112"}},"outputId":"a1031819-71bf-40d6-db49-0fc87210b94d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"AMynsCKikld_"},"source":["### Functions for WVS Dataset"]},{"cell_type":"code","source":["#TOKEN_PAIRS = [ ('always justifiable', 'never justifiable')]\n","\n","TOKEN_PAIRS = [ ('always justifiable', 'never justifiable'),\n","                ('right','wrong'),\n","                ('morally good','morally bad'),\n","                ('ethically right', 'ethically wrong'),\n","                ('ethical','unethical')]"],"metadata":{"id":"ZnSwkvcIdUsc"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cj-8YoTYNONa"},"outputs":[],"source":["def get_wvs_df():\n","    #wvs_df = pd.read_csv('/content/drive/MyDrive/MSc Applied Data Science/ADS thesis Mijntje/WVS_Moral.csv') #WVS_Moral is a subset of the full data for just the moral questions\n","    wvs_df = pd.read_csv('/content/drive/MyDrive/ADS thesis Mijntje/WVS_Cross-National_Wave_7_csv_v5_0.csv')\n","    filtered_columns = wvs_df.filter(regex='^(Q17[7-9]|Q18[0-9]|Q19[0-5]|B_COUNTRY)$')\n","    wvs_df = wvs_df[filtered_columns.columns]\n","    wvs_df_country_names = pd.read_csv('/content/drive/MyDrive/MSc Applied Data Science/ADS thesis Mijntje/Country_Codes_Names-correct.csv')\n","    wvs_df = wvs_df.set_index('B_COUNTRY').join(wvs_df_country_names.set_index('B_COUNTRY'), how='left')\n","    return wvs_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oa6mqmhjNfon"},"outputs":[],"source":["COUNTRIES_WVS_W7 = ['Andorra'] #subset of countries for testing\n","\n","COUNTRIES_WVS_W7_ALL = [\n","    'Andorra', 'Argentina', 'Armenia', 'Australia', 'Bangladesh', 'Bolivia', 'Brazil', 'Canada',\n","    'Chile', 'China', 'Colombia', 'Cyprus', 'Ecuador', 'Egypt', 'Ethiopia', 'Germany', 'Greece',\n","    'Guatemala', 'Indonesia', 'Iran', 'Iraq', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya',\n","    'Kyrgyzstan', 'Lebanon', 'Libya', 'Malaysia', 'Maldives', 'Mexico', 'Mongolia', 'Morocco',\n","    'Myanmar', 'Netherlands', 'New Zealand', 'Nicaragua', 'Nigeria', 'Pakistan', 'Peru',\n","    'Philippines', 'Romania', 'Russia', 'Singapore', 'South Korea', 'Taiwan ROC', 'Tajikistan',\n","    'Thailand', 'Tunisia', 'Turkey', 'Ukraine', 'United States', 'Venezuela',\n","    'Vietnam', 'Zimbabwe'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CWSwr9ZdNgU7"},"outputs":[],"source":["W7_QUESTIONS = ['Q'+str(i) for i in range(177, 196)]\n","\n","W7_QUESTIONS_TEXT = ['claiming government benefits to which you are not entitled',\n","                     'avoiding a fare on public transport',\n","                     'stealing property',\n","                     'cheating on taxes',\n","                     'someone accepting a bribe in the course of their duties',\n","                     'homosexuality',\n","                     'prostitution',\n","                     'abortion',\n","                     'divorce',\n","                     'sex before marriage',\n","                     'suicide',\n","                     'euthanasia',\n","                     'for a man to beat his wife',\n","                     'parents beating children',\n","                     'violence against other people',\n","                     'terrorism as a political, ideological or religious mean',\n","                     'having casual sex',\n","                     'political violence',\n","                     'death penalty']\n","\n","QUESTION_WAVES_WVS = {7: (W7_QUESTIONS, W7_QUESTIONS_TEXT)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yYEDuWuTNwjS"},"outputs":[],"source":["MINUS = 5.5\n","DIVIDE = 4.5\n","\n","def get_wvs_ratings(wvs_df, culture, q):\n","    df = wvs_df[['Country_Names', q]]\n","    df = df.loc[df['Country_Names'] == culture]\n","    if len(df) == 0:\n","        return None\n","\n","    ratings = df.loc[df[q] > 0][q]\n","    if ratings.empty:\n","        ratings = 0\n","    else:\n","      ratings = ((ratings - MINUS) / DIVIDE).mean()\n","    return ratings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CMWvZMUbN1c1"},"outputs":[],"source":["#Generating the WVS prompts\n","\n","def wvs_gpt2_prompts_ratings_multiple_tokens(wvs_df:pd.DataFrame, culture ='', prompt_mode ='in', wave = 7, ):\n","    prompts = {}\n","    culture_prompt = ''\n","    if len(culture) > 0:\n","        if prompt_mode == 'in':\n","            print(\"The token used is IN\")\n","            culture_prompt = f'In {culture} '\n","        elif prompt_mode == 'people':\n","            print(\"The token used is PEOPLE\")\n","            culture_prompt = f'People in {culture} believe '\n","\n","    questions, questions_text = QUESTION_WAVES_WVS[wave]\n","\n","    for q,q_text in zip(questions, questions_text):\n","        rating_answers = get_wvs_ratings(wvs_df, culture, q) #getting the original ratings\n","\n","        prompts[q_text] = []\n","        for (prompt_head_moral, prompt_head_nonmoral) in TOKEN_PAIRS:\n","            prompt_moral = f'{culture_prompt}{q_text} is {prompt_head_moral}.'\n","            prompt_nonmoral = f'{culture_prompt}{q_text} is {prompt_head_nonmoral}.'\n","\n","            prompts[q_text].append((prompt_moral, prompt_nonmoral,rating_answers))\n","\n","    return prompts"]},{"cell_type":"code","source":["wvs_df = get_wvs_df()  #!!!!!!!!!!!!!!!!!!!!!!\n","    # pew_df = get_pew_df()    #!!!!!!!!!!!!!!!!!!!!!!\n","\n","# cultures.append('') # Comment this line if universal scores are not needed\n","wvs_all = []\n","\n","cultures = COUNTRIES_WVS_W7_ALL\n","\n","for culture in tqdm(cultures):\n","  # if culture in excluding_cultures:\n","  #     continue\n","  prompts = wvs_gpt2_prompts_ratings_multiple_tokens(wvs_df, culture, 'in')\n","  # prompts = pew_gpt2_prompts_ratings_multiple_tokens(pew_df, culture, prompt_mode)  #!!!!!!!!!!!!!!!!!\n","\n","  culture_name = culture if culture else 'universal'\n","  for question, rating_pairs in prompts.items():\n","      # if any(excluded_topic in question for excluded_topic in excluding_topics):\n","      #   continue\n","\n","      wvs_score = rating_pairs[0][2]  #Assuming all pairs have the same wvs_score\n","\n","      row = {\n","              'country': culture_name, 'topic': question, 'wvs_score': wvs_score\n","            }\n","\n","      wvs_all.append(row)\n","\n","df = pd.DataFrame(wvs_all)\n","\n","save_dir = f'/content/drive/MyDrive/MSc Applied Data Science/ADS thesis Mijntje/WVS_moral_scores.csv'\n","df.to_csv(save_dir, index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nyDwp6vVRqQG","executionInfo":{"status":"ok","timestamp":1719532211139,"user_tz":-120,"elapsed":15730,"user":{"displayName":"Mijntje Meijer","userId":"17018801464945965112"}},"outputId":"d743b02e-34ca-4bd4-da59-f41f42ab8e06"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-46-0cc0b8edc23c>:3: DtypeWarning: Columns (345,347,349,574,575,576) have mixed types. Specify dtype option on import or set low_memory=False.\n","  wvs_df = pd.read_csv('/content/drive/MyDrive/ADS thesis/cultural_inference-main/data/WVS/WVS_Cross-National_Wave_7_csv_v5_0.csv')\n","  0%|          | 0/55 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r  2%|▏         | 1/55 [00:00<00:14,  3.62it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▎         | 2/55 [00:00<00:15,  3.51it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r  5%|▌         | 3/55 [00:00<00:14,  3.48it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r  7%|▋         | 4/55 [00:01<00:14,  3.53it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r  9%|▉         | 5/55 [00:01<00:14,  3.52it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r 11%|█         | 6/55 [00:01<00:14,  3.50it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r 13%|█▎        | 7/55 [00:02<00:13,  3.48it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r 15%|█▍        | 8/55 [00:02<00:13,  3.46it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 18%|█▊        | 10/55 [00:02<00:11,  4.04it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 12/55 [00:03<00:08,  4.90it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 25%|██▌       | 14/55 [00:03<00:07,  5.50it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 29%|██▉       | 16/55 [00:03<00:06,  5.62it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 18/55 [00:04<00:06,  5.85it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 36%|███▋      | 20/55 [00:04<00:05,  5.98it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 22/55 [00:04<00:05,  5.95it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 44%|████▎     | 24/55 [00:05<00:05,  6.08it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 47%|████▋     | 26/55 [00:05<00:04,  6.14it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 51%|█████     | 28/55 [00:05<00:04,  5.99it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 55%|█████▍    | 30/55 [00:06<00:04,  6.08it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 58%|█████▊    | 32/55 [00:06<00:03,  6.09it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 62%|██████▏   | 34/55 [00:06<00:03,  5.76it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 65%|██████▌   | 36/55 [00:07<00:03,  5.94it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 69%|██████▉   | 38/55 [00:07<00:02,  6.01it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 73%|███████▎  | 40/55 [00:07<00:02,  5.98it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 76%|███████▋  | 42/55 [00:08<00:02,  5.87it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r 78%|███████▊  | 43/55 [00:08<00:02,  5.89it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 44/55 [00:08<00:02,  5.42it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 84%|████████▎ | 46/55 [00:08<00:01,  5.19it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 87%|████████▋ | 48/55 [00:09<00:01,  5.41it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 91%|█████████ | 50/55 [00:09<00:00,  5.56it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\r 93%|█████████▎| 51/55 [00:09<00:00,  5.65it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":[" 96%|█████████▋| 53/55 [00:10<00:00,  5.38it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n","The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 55/55 [00:10<00:00,  5.24it/s]"]},{"output_type":"stream","name":"stdout","text":["The token used is IN\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"markdown","metadata":{"id":"wgZRPgi4dT5E"},"source":["### Functions for PEW Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"to0NqVG6QVqv"},"outputs":[],"source":["#1\n","import pandas as pd\n","\n","def get_pew_df():\n","    # Load data using pandas read_spss (make sure the path is correctly specified)\n","    pew_data_original = pd.read_spss('/content/drive/MyDrive/MSc Applied Data Science/ADS thesis Mijntje/Pew Research Global Attitudes Project Spring 2013 Dataset for web.sav')\n","\n","    # Filter columns using regex and directly create a new DataFrame\n","    filtered_columns = pew_data_original.filter(regex='^Q84[A-H]|COUNTRY').copy()\n","\n","    # Rename 'COUNTRY' column to 'Country_Names'\n","    filtered_columns.rename(columns={'COUNTRY': 'Country_Names'}, inplace=True)\n","\n","    # Define a mapping dictionary to replace strings with numeric values\n","    replace_map = {\n","        'Morally acceptable': 1,\n","        'Not a moral issue': 0,\n","        'Morally unacceptable': -1,\n","        'Depends on situation (Volunteered)': 0,\n","        'Refused': 0,\n","        \"Don't know\": 0\n","    }\n","\n","    # Apply the replacement map to the DataFrame\n","    filtered_columns.replace(replace_map, inplace=True)\n","\n","    # Convert all columns (except 'Country_Names') to numeric\n","    for col in filtered_columns.columns[1:]:\n","        filtered_columns[col] = pd.to_numeric(filtered_columns[col], errors='coerce')\n","\n","    return filtered_columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FYbTroBLlCnK"},"outputs":[],"source":["#2 DONE!\n","COUNTRIES_PEW_ALL = [\n","    'United States', 'Czech Republic', 'South Korea', 'Canada', 'France', 'Germany',\n","    'Spain', 'Mexico', 'Chile', 'Australia', 'Russia', 'Britain', 'Turkey', 'Greece',\n","    'Egypt', 'Poland', 'Senegal', 'Italy', 'Brazil', 'Lebanon', 'Nigeria', 'Japan',\n","    'Malaysia', 'Kenya', 'Indonesia', 'Uganda', 'Jordan', 'Argentina', 'Philippines',\n","    'Tunisia', 'China', 'Pakistan', 'Ghana', 'South Africa', 'Palestinian territories',\n","    'Israel', 'Bolivia', 'Venezuela', 'El Salvador'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lDs6XtH2lZXG"},"outputs":[],"source":["#3 DONE!\n","PEW_QUESTIONS = ['Q84' + chr(i) for i in range(ord('A'), ord('H')+1)]\n","\n","PEW_QUESTIONS_TEXT = ['using contraceptives',\n","                      'getting a divorce',\n","                      'having an abortion',\n","                      'homosexuality',\n","                      'drinking alcohol',\n","                      'married people having an affair',\n","                      'gambling',\n","                      'sex between unmarried adults']\n","\n","#DIFFERENT NAME, ADDED PEW\n","QUESTION_WAVES_PEW = {13: (PEW_QUESTIONS, PEW_QUESTIONS_TEXT)}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aqBzdaw0oOcX"},"outputs":[],"source":["#4 done!\n","#My values are already normalized\n","\n","def get_pew_ratings(pew_df, culture, q):\n","    df = pew_df[['Country_Names', q]]\n","    df = df.loc[df['Country_Names'] == culture]\n","    if df.empty:\n","        print(\"No data found for culture:\", culture)\n","        return None\n","\n","    # Remove any non-positive values if similar to the previous logic is needed\n","    # df = df[df[q] > 0]\n","\n","    # Compute the mean of the ratings\n","    mean_rating = df[q].mean()\n","\n","    if pd.isna(mean_rating):\n","        print(\"Problem: Mean calculation resulted in NaN for culture:\", culture)\n","        return None\n","\n","    return mean_rating"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KeuGP2bUoOkW"},"outputs":[],"source":["#5\n","#Generating the PEW prompts\n","def pew_gpt2_prompts_ratings_multiple_tokens(pew_df:pd.DataFrame, culture ='', prompt_mode ='in', wave = 13, ):\n","    prompts = {}\n","    culture_prompt = ''\n","    if len(culture) > 0:\n","        if prompt_mode == 'in':\n","            culture_prompt = f'In {culture} '\n","            print(\"Inside IN\")\n","        elif prompt_mode == 'people':\n","            culture_prompt = f'People in {culture} believe '\n","            print(\"Inside PEOPLE\")\n","\n","    questions, questions_text = QUESTION_WAVES_PEW[wave]\n","\n","    for q,q_text in zip(questions, questions_text):\n","        rating_answers = get_pew_ratings(pew_df, culture, q) #getting the original ratings\n","\n","        prompts[q_text] = []\n","        for (prompt_head_moral, prompt_head_nonmoral) in TOKEN_PAIRS:\n","            prompt_moral = f'{culture_prompt}{q_text} is {prompt_head_moral}.'\n","            prompt_nonmoral = f'{culture_prompt}{q_text} is {prompt_head_nonmoral}.'\n","\n","            prompts[q_text].append((prompt_moral, prompt_nonmoral,rating_answers))\n","\n","    return prompts"]},{"cell_type":"code","source":["# wvs_df = get_wvs_df()  #!!!!!!!!!!!!!!!!!!!!!!\n","pew_df = get_pew_df()    #!!!!!!!!!!!!!!!!!!!!!!\n","\n","# cultures.append('') # Comment this line if universal scores are not needed\n","pew_all = []\n","\n","cultures = COUNTRIES_PEW_ALL\n","\n","for culture in tqdm(cultures):\n","  # if culture in excluding_cultures:\n","  #     continue\n","  #prompts = wvs_gpt2_prompts_ratings_multiple_tokens(wvs_df, culture, 'in')\n","  prompts = pew_gpt2_prompts_ratings_multiple_tokens(pew_df, culture, prompt_mode='in')  #!!!!!!!!!!!!!!!!!\n","\n","  culture_name = culture if culture else 'universal'\n","  for question, rating_pairs in prompts.items():\n","      # if any(excluded_topic in question for excluded_topic in excluding_topics):\n","      #   continue\n","\n","      pew_score = rating_pairs[0][2]  #Assuming all pairs have the same wvs_score\n","\n","      row = {\n","              'country': culture_name, 'topic': question, 'pew_score': pew_score\n","            }\n","\n","      pew_all.append(row)\n","\n","df = pd.DataFrame(pew_all)\n","\n","save_dir = f'/content/drive/MyDrive/MSc Applied Data Science/ADS thesis Mijntje/PEW_moral_scores.csv'\n","df.to_csv(save_dir, index=False)"],"metadata":{"id":"e1rDinWGYgdv","executionInfo":{"status":"ok","timestamp":1719532224034,"user_tz":-120,"elapsed":12542,"user":{"displayName":"Mijntje Meijer","userId":"17018801464945965112"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"929035c0-cd53-49da-d35f-49fc7a0c2247"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 26%|██▌       | 10/39 [00:00<00:00, 98.97it/s]"]},{"output_type":"stream","name":"stdout","text":["Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 39/39 [00:00<00:00, 96.83it/s]"]},{"output_type":"stream","name":"stdout","text":["Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n","Inside IN\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ZmuZFpuMGxJD"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1pbuql1pSd6oJQD_BJKF5OTB2udP1_a1K","timestamp":1719530113232},{"file_id":"15yhJviYmgxFKRFLLnOKdt9fpjo4hFxZJ","timestamp":1718807548472}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}