{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roUMVnZxF1Yk"
      },
      "source": [
        "#Common functions for all models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0YMLS_IF6yS"
      },
      "source": [
        "### Installing Packages & Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNd1w4tXAzr3"
      },
      "outputs": [],
      "source": [
        "!pip install pyreadstat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iMbo1Nd-F7ds"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import transformers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from scipy.stats import pearsonr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6-0jkmYGucL"
      },
      "source": [
        "### Functions for WVS Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fr7Fn5sbGu_g"
      },
      "outputs": [],
      "source": [
        "#Loading the WVS file and storing it in a dataframe format\n",
        "\n",
        "def get_wvs_df():\n",
        "    wvs_df = pd.read_csv('sample_data/WVS_Moral.csv') #WVS_Moral is a subset of the full data for just the moral questions\n",
        "    wvs_df_country_names = pd.read_csv('sample_data/Country_Codes_Names.csv')\n",
        "    wvs_df = wvs_df.set_index('B_COUNTRY').join(wvs_df_country_names.set_index('B_COUNTRY'), how='left')\n",
        "    return wvs_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "MUE0vGtNGxZZ"
      },
      "outputs": [],
      "source": [
        "#Listing all the countries we are gonna use in our study from the WVS dataset\n",
        "\n",
        "COUNTRIES_WVS_W7_ALL = [\n",
        "    'Andorra', 'Argentina', 'Armenia', 'Australia', 'Bangladesh', 'Bolivia', 'Brazil', 'Canada',\n",
        "    'Chile', 'China', 'Colombia', 'Cyprus', 'Ecuador', 'Egypt', 'Ethiopia', 'Germany', 'Greece',\n",
        "    'Guatemala', 'Indonesia', 'Iran', 'Iraq', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya',\n",
        "    'Kyrgyzstan', 'Lebanon', 'Libya', 'Malaysia', 'Maldives', 'Mexico', 'Mongolia', 'Morocco',\n",
        "    'Myanmar', 'Netherlands', 'New Zealand', 'Nicaragua', 'Nigeria', 'Pakistan', 'Peru',\n",
        "    'Philippines', 'Romania', 'Russia', 'Singapore', 'South Korea', 'Taiwan ROC', 'Tajikistan',\n",
        "    'Thailand', 'Tunisia', 'Turkey', 'Ukraine', 'United States', 'Venezuela',\n",
        "    'Vietnam', 'Zimbabwe'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jBm5455PHDsB"
      },
      "outputs": [],
      "source": [
        "#Creating the questions for the WVS dataset\n",
        "\n",
        "W7_QUESTIONS = ['Q'+str(i) for i in range(177, 196)]\n",
        "\n",
        "W7_QUESTIONS_TEXT = ['claiming government benefits to which you are not entitled',\n",
        "                     'avoiding a fare on public transport',\n",
        "                     'stealing property',\n",
        "                     'cheating on taxes',\n",
        "                     'someone accepting a bribe in the course of their duties',\n",
        "                     'homosexuality',\n",
        "                     'prostitution',\n",
        "                     'abortion',\n",
        "                     'divorce',\n",
        "                     'sex before marriage',\n",
        "                     'suicide',\n",
        "                     'euthanasia',\n",
        "                     'for a man to beat his wife',\n",
        "                     'parents beating children',\n",
        "                     'violence against other people',\n",
        "                     'terrorism as a political, ideological or religious mean',\n",
        "                     'having casual sex',\n",
        "                     'political violence',\n",
        "                     'death penalty']\n",
        "\n",
        "QUESTION_WAVES_WVS = {7: (W7_QUESTIONS, W7_QUESTIONS_TEXT)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "LV_014_lHXLC"
      },
      "outputs": [],
      "source": [
        "#Scaling the survey results between -1 and 1 and calculating the mean rating for a every survey question and culture\n",
        "\n",
        "MINUS = 5.5\n",
        "DIVIDE = 4.5\n",
        "\n",
        "def get_wvs_ratings(wvs_df, culture, q):\n",
        "    df = wvs_df[['Country_Names', q]]\n",
        "    df = df.loc[df['Country_Names'] == culture]\n",
        "    if len(df) == 0:\n",
        "        return None\n",
        "\n",
        "    ratings = df.loc[df[q] > 0][q]\n",
        "    if ratings.empty:\n",
        "        ratings = 0\n",
        "    else:\n",
        "      ratings = ((ratings - MINUS) / DIVIDE).mean()\n",
        "    return ratings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "H3Nfbz0THpPe"
      },
      "outputs": [],
      "source": [
        "#Generating the WVS prompts\n",
        "\n",
        "def wvs_gpt2_prompts_ratings_multiple_tokens(wvs_df:pd.DataFrame, culture ='', prompt_mode ='in', wave = 7, ):\n",
        "    prompts = {}\n",
        "    culture_prompt = ''\n",
        "    if len(culture) > 0:\n",
        "        if prompt_mode == 'in':\n",
        "            print(\"The token used is IN\")\n",
        "            culture_prompt = f'In {culture} '\n",
        "        elif prompt_mode == 'people':\n",
        "            print(\"The token used is PEOPLE\")\n",
        "            culture_prompt = f'People in {culture} believe '\n",
        "\n",
        "    questions, questions_text = QUESTION_WAVES_WVS[wave]\n",
        "\n",
        "    for q,q_text in zip(questions, questions_text):\n",
        "        rating_answers = get_wvs_ratings(wvs_df, culture, q) #getting the original ratings\n",
        "\n",
        "        prompts[q_text] = []\n",
        "        for (prompt_head_moral, prompt_head_nonmoral) in TOKEN_PAIRS:\n",
        "            prompt_moral = f'{culture_prompt}{q_text} is {prompt_head_moral}.'\n",
        "            prompt_nonmoral = f'{culture_prompt}{q_text} is {prompt_head_nonmoral}.'\n",
        "\n",
        "            prompts[q_text].append((prompt_moral, prompt_nonmoral,rating_answers))\n",
        "\n",
        "    return prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trRrPYkqHq82"
      },
      "source": [
        "### Functions for PEW Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "x73_wwe7Hs1o"
      },
      "outputs": [],
      "source": [
        "#Loading the PEW file, pre-processing it and storing it in a dataframe\n",
        "\n",
        "def get_pew_df():\n",
        "    pew_data_original = pd.read_spss('sample_data/Pew Research Global Attitudes Project Spring 2013 Dataset for web.sav')\n",
        "\n",
        "    filtered_columns = pew_data_original.filter(regex='^Q84[A-H]|COUNTRY').copy()\n",
        "\n",
        "    filtered_columns.rename(columns={'COUNTRY': 'Country_Names'}, inplace=True)\n",
        "\n",
        "    replace_map = {\n",
        "        'Morally acceptable': 1,\n",
        "        'Not a moral issue': 0,\n",
        "        'Morally unacceptable': -1,\n",
        "        'Depends on situation (Volunteered)': 0,\n",
        "        'Refused': 0,\n",
        "        \"Don't know\": 0\n",
        "    }\n",
        "\n",
        "    filtered_columns.replace(replace_map, inplace=True)\n",
        "\n",
        "    for col in filtered_columns.columns[1:]:\n",
        "        filtered_columns[col] = pd.to_numeric(filtered_columns[col], errors='coerce')\n",
        "\n",
        "    return filtered_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e9jI844nI-jr"
      },
      "outputs": [],
      "source": [
        "#Listing all the countries we are gonna use in our study from the PEW dataset\n",
        "\n",
        "COUNTRIES_PEW_ALL = [\n",
        "    'United States', 'Czech Republic', 'South Korea', 'Canada', 'France', 'Germany',\n",
        "    'Spain', 'Mexico', 'Chile', 'Australia', 'Russia', 'Britain', 'Turkey', 'Greece',\n",
        "    'Egypt', 'Poland', 'Senegal', 'Italy', 'Brazil', 'Lebanon', 'Nigeria', 'Japan',\n",
        "    'Malaysia', 'Kenya', 'Indonesia', 'Uganda', 'Jordan', 'Argentina', 'Philippines',\n",
        "    'Tunisia', 'China', 'Pakistan', 'Ghana', 'South Africa', 'Palestinian territories',\n",
        "    'Israel', 'Bolivia', 'Venezuela', 'El Salvador'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5Kdml1PIJIYm"
      },
      "outputs": [],
      "source": [
        "#Creating the questions for the PEW dataset\n",
        "\n",
        "PEW_QUESTIONS = ['Q84' + chr(i) for i in range(ord('A'), ord('H')+1)]\n",
        "\n",
        "PEW_QUESTIONS_TEXT = ['using contraceptives',\n",
        "                      'getting a divorce',\n",
        "                      'having an abortion',\n",
        "                      'homosexuality',\n",
        "                      'drinking alcohol',\n",
        "                      'married people having an affair',\n",
        "                      'gambling',\n",
        "                      'sex between unmarried adults']\n",
        "\n",
        "QUESTION_WAVES_PEW = {13: (PEW_QUESTIONS, PEW_QUESTIONS_TEXT)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RgRKrgYvSa_X"
      },
      "outputs": [],
      "source": [
        "#Calculating the mean rating for a every question and culture\n",
        "\n",
        "def get_pew_ratings(pew_df, culture, q):\n",
        "    df = pew_df[['Country_Names', q]]\n",
        "    df = df.loc[df['Country_Names'] == culture]\n",
        "    if df.empty:\n",
        "        print(\"No data found for culture:\", culture)\n",
        "        return None\n",
        "\n",
        "    mean_rating = df[q].mean()\n",
        "\n",
        "    if pd.isna(mean_rating):\n",
        "        print(\"Problem: Mean calculation resulted in NaN for culture:\", culture)\n",
        "        return None\n",
        "\n",
        "    return mean_rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SrdP-lBHSbDE"
      },
      "outputs": [],
      "source": [
        "#Generating the PEW prompts\n",
        "\n",
        "def pew_gpt2_prompts_ratings_multiple_tokens(pew_df:pd.DataFrame, culture ='', prompt_mode ='in', wave = 13, ):\n",
        "    prompts = {}\n",
        "    culture_prompt = ''\n",
        "    if len(culture) > 0:\n",
        "        if prompt_mode == 'in':\n",
        "            culture_prompt = f'In {culture} '\n",
        "            print(\"Inside IN\")\n",
        "        elif prompt_mode == 'people':\n",
        "            culture_prompt = f'People in {culture} believe '\n",
        "            print(\"Inside PEOPLE\")\n",
        "\n",
        "    questions, questions_text = QUESTION_WAVES_PEW[wave]\n",
        "\n",
        "    for q,q_text in zip(questions, questions_text):\n",
        "        rating_answers = get_pew_ratings(pew_df, culture, q) #getting the original ratings\n",
        "\n",
        "        prompts[q_text] = []\n",
        "        for (prompt_head_moral, prompt_head_nonmoral) in TOKEN_PAIRS:\n",
        "            prompt_moral = f'{culture_prompt}{q_text} is {prompt_head_moral}.'\n",
        "            prompt_nonmoral = f'{culture_prompt}{q_text} is {prompt_head_nonmoral}.'\n",
        "\n",
        "            prompts[q_text].append((prompt_moral, prompt_nonmoral,rating_answers))\n",
        "\n",
        "    return prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ay6F1FPdPZZ"
      },
      "source": [
        "## Other functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SClE4R6mmJLm"
      },
      "source": [
        "### normalize_log_prob_diffs() and calculate_correlation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hCKDxhlSmJUX"
      },
      "outputs": [],
      "source": [
        "#Normalizing the log probability differences in a scale between -1 and 1\n",
        "\n",
        "def normalize_log_prob_diffs(log_prob_diffs):\n",
        "    min_log_prob = np.min(log_prob_diffs)\n",
        "    max_log_prob = np.max(log_prob_diffs)\n",
        "    normalized_log_probs = 2 * (log_prob_diffs - min_log_prob) / (max_log_prob - min_log_prob) - 1\n",
        "    return normalized_log_probs\n",
        "\n",
        "#Calculating the Pearson correlation between model and survey scores\n",
        "\n",
        "def calculate_correlation(survey_scores, log_prob_diffs):\n",
        "  if len(survey_scores) != len(log_prob_diffs):\n",
        "      print(f\"Error: Mismatched lengths. Survey scores length: {len(survey_scores)}, Log prob diffs length: {len(log_prob_diffs)}\")\n",
        "      return None, None, None\n",
        "\n",
        "  normalized_log_probs = normalize_log_prob_diffs(log_prob_diffs)\n",
        "  try:\n",
        "        correlation, p_value = pearsonr(survey_scores, normalized_log_probs)\n",
        "  except Exception as e:\n",
        "        print(\"Error during correlation calculation:\", e)\n",
        "        return None, None, None\n",
        "  return correlation, normalized_log_probs, p_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaeQRYO4kV2s"
      },
      "source": [
        "### get_batch_last_token_log_prob() function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "bunjLFpIkWAp"
      },
      "outputs": [],
      "source": [
        "#Calculating the log probabilities of each of the 2 tokens\n",
        "\n",
        "def get_batch_last_token_log_prob(lines, model, tokenizer, use_cuda=False, end_with_period=True):\n",
        "    eos_token = tokenizer.eos_token or tokenizer.sep_token\n",
        "    if eos_token is None:\n",
        "        raise ValueError(\"Neither eos_token nor sep_token is set in the tokenizer.\")\n",
        "\n",
        "    lines = [line + eos_token for line in lines]  #Appending EOS to the end of each line\n",
        "\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    tok_moral = tokenizer.batch_encode_plus(lines, return_tensors='pt', padding='longest', add_special_tokens=True)\n",
        "    input_ids = tok_moral['input_ids']\n",
        "    attention_mask = tok_moral['attention_mask']\n",
        "    lines_len = torch.sum(attention_mask, dim=1)\n",
        "\n",
        "    remove_from_end = 2 if end_with_period else 1  #If there is a period remove it as well as the EOS token else remove only the EOS token\n",
        "    tokens_wanted = lines_len - remove_from_end\n",
        "\n",
        "    if use_cuda:\n",
        "        device = next(model.parameters()).device\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids, return_dict=True)\n",
        "        logits = outputs.logits\n",
        "        if use_cuda:\n",
        "            logits = logits.detach().cpu()\n",
        "\n",
        "    logits_probs = F.log_softmax(logits, dim=-1)\n",
        "\n",
        "    batch_indices = torch.arange(input_ids.size(0))\n",
        "    token_indices = tokens_wanted - 1   #take the index of the token we need. index of 1st element is 0 so we have to do -1\n",
        "    next_token_indices = input_ids[batch_indices, tokens_wanted].cpu()\n",
        "\n",
        "    log_probs = logits_probs[batch_indices, token_indices, next_token_indices]\n",
        "\n",
        "    return log_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poNF1tlMkElB"
      },
      "source": [
        "### get_log_prob_difference() function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WjbO0ZH4bkKp"
      },
      "outputs": [],
      "source": [
        "#Calculating the average log probability differences between moral and non-moral prompts\n",
        "\n",
        "def get_log_prob_difference(pairs, moral_index, nonmoral_index, model, tokenizer, use_cuda):\n",
        "    question_average_lm_score = []\n",
        "    average_moral_score = []\n",
        "    average_nonmoral_score = []\n",
        "\n",
        "    all_prompts = []\n",
        "    for rating in pairs:\n",
        "        moral_prompt = rating[moral_index]\n",
        "        nonmoral_prompt = rating[nonmoral_index]\n",
        "        all_prompts.append(moral_prompt)\n",
        "        all_prompts.append(nonmoral_prompt)\n",
        "\n",
        "    logprobs = get_batch_last_token_log_prob(all_prompts, model, tokenizer, use_cuda)\n",
        "\n",
        "    for i in range(0, len(logprobs), 2):\n",
        "        moral_logprob = logprobs[i]\n",
        "        nonmoral_logprob = logprobs[i + 1]\n",
        "        lm_score = moral_logprob - nonmoral_logprob\n",
        "\n",
        "        question_average_lm_score.append(lm_score)\n",
        "        average_moral_score.append(moral_logprob)\n",
        "        average_nonmoral_score.append(nonmoral_logprob)\n",
        "\n",
        "    lm_score = np.mean(question_average_lm_score)\n",
        "    moral_score = np.mean(average_moral_score)\n",
        "    nonmoral_score = np.mean(average_nonmoral_score)\n",
        "\n",
        "    return lm_score, moral_score, nonmoral_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsvE58Gvi2Ev"
      },
      "source": [
        "### compare_token_pair() functions for both datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ar6miXzgdGN1"
      },
      "outputs": [],
      "source": [
        "def compare_wvs_token_pairs(model_name, cultures=None, wave=7, excluding_topics=[],\n",
        "                            excluding_cultures=[], model=None, tokenizer=None, use_cuda=False, prompt_mode='in'):\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "        model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        if use_cuda:\n",
        "            model = model.cuda()\n",
        "\n",
        "    wvs_df = get_wvs_df()\n",
        "\n",
        "    results_all = []\n",
        "\n",
        "    for culture in tqdm(cultures):\n",
        "        if culture in excluding_cultures:\n",
        "            continue\n",
        "        prompts = wvs_gpt2_prompts_ratings_multiple_tokens(wvs_df, culture, prompt_mode)\n",
        "\n",
        "        culture_name = culture if culture else 'universal'\n",
        "        for question, rating_pairs in prompts.items():\n",
        "            if any(excluded_topic in question for excluded_topic in excluding_topics):\n",
        "                continue\n",
        "\n",
        "            lm_score, moral_log_probs, nonmoral_log_probs = get_log_prob_difference(rating_pairs, 0, 1, model, tokenizer, use_cuda)\n",
        "\n",
        "            if isinstance(moral_log_probs, torch.Tensor):\n",
        "              moral_log_probs = moral_log_probs.item()\n",
        "            if isinstance(nonmoral_log_probs, torch.Tensor):\n",
        "              nonmoral_log_probs = nonmoral_log_probs.item()\n",
        "            if isinstance(lm_score, torch.Tensor):\n",
        "              lm_score = lm_score.item()\n",
        "\n",
        "            wvs_score = rating_pairs[0][2]  #All token pairs have the same wvs_score\n",
        "\n",
        "            row = {\n",
        "                'country': culture_name, 'topic': question, 'wvs_score': wvs_score,\n",
        "                'moral log prob': moral_log_probs, 'non moral log probs': nonmoral_log_probs,\n",
        "                'log prob difference': lm_score\n",
        "            }\n",
        "\n",
        "            results_all.append(row)\n",
        "\n",
        "    df = pd.DataFrame(results_all)\n",
        "\n",
        "    survey_scores = df['wvs_score'].values\n",
        "    log_prob_diffs = df['log prob difference'].values\n",
        "\n",
        "    correlation, normalized_log_probs, p_value = calculate_correlation(survey_scores, log_prob_diffs)\n",
        "\n",
        "    df['normalized log prob difference'] = normalized_log_probs\n",
        "\n",
        "    metrics = {\n",
        "        'Pearson correlation coefficient': correlation,\n",
        "        'P value': p_value\n",
        "    }\n",
        "\n",
        "    return df, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bzEtJhpZipTq"
      },
      "outputs": [],
      "source": [
        "def compare_pew_token_pairs(model_name, cultures=None, wave=7, excluding_topics=[],\n",
        "                            excluding_cultures=[], model=None, tokenizer=None, use_cuda=False, prompt_mode='in'):\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
        "        model = transformers.AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        if use_cuda:\n",
        "            model = model.cuda()\n",
        "\n",
        "    pew_df = get_pew_df()\n",
        "\n",
        "    results_all = []\n",
        "\n",
        "    for culture in tqdm(cultures):\n",
        "        if culture in excluding_cultures:\n",
        "            continue\n",
        "        prompts = pew_gpt2_prompts_ratings_multiple_tokens(pew_df, culture, prompt_mode)\n",
        "\n",
        "        culture_name = culture if culture else 'universal'\n",
        "        for question, rating_pairs in prompts.items():\n",
        "            if any(excluded_topic in question for excluded_topic in excluding_topics):\n",
        "                continue\n",
        "\n",
        "            lm_score, moral_log_probs, nonmoral_log_probs = get_log_prob_difference(rating_pairs, 0, 1, model, tokenizer, use_cuda)\n",
        "\n",
        "            if isinstance(moral_log_probs, torch.Tensor):\n",
        "              moral_log_probs = moral_log_probs.item()\n",
        "            if isinstance(nonmoral_log_probs, torch.Tensor):\n",
        "              nonmoral_log_probs = nonmoral_log_probs.item()\n",
        "            if isinstance(lm_score, torch.Tensor):\n",
        "              lm_score = lm_score.item()\n",
        "\n",
        "            pew_score = rating_pairs[0][2]  #All token pairs have the same pew_score\n",
        "\n",
        "            row = {\n",
        "                'country': culture_name, 'topic': question, 'pew_score': pew_score,\n",
        "                'moral log prob': moral_log_probs, 'non moral log probs': nonmoral_log_probs,\n",
        "                'log prob difference': lm_score\n",
        "            }\n",
        "\n",
        "            results_all.append(row)\n",
        "\n",
        "    df = pd.DataFrame(results_all)\n",
        "\n",
        "    survey_scores = df['pew_score'].values\n",
        "    log_prob_diffs = df['log prob difference'].values\n",
        "\n",
        "    correlation, normalized_log_probs, p_value = calculate_correlation(survey_scores, log_prob_diffs)\n",
        "\n",
        "    df['normalized log prob difference'] = normalized_log_probs\n",
        "\n",
        "    metrics = {\n",
        "        'Pearson correlation coefficient': correlation,\n",
        "        'P value': p_value\n",
        "    }\n",
        "\n",
        "    return df, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xlVS8564zLO"
      },
      "source": [
        "## Functions only for GPT2 models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "jqszHz5n42V6"
      },
      "outputs": [],
      "source": [
        "#Loading the GPT2 model and the tokenizer\n",
        "\n",
        "def get_gpt2_model(modelname = 'gpt2', use_cuda = True, device = 'cuda:0'):  #'gpt2','gpt2-medium','gpt2-large','gpt2-xl'\n",
        "    model = GPT2LMHeadModel.from_pretrained(modelname)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(modelname)\n",
        "\n",
        "    if use_cuda and torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        device = torch.device(device)\n",
        "        model.cuda(device)\n",
        "\n",
        "    return tokenizer, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "p93y0Jla44Tk"
      },
      "outputs": [],
      "source": [
        "def compare_wvs_token_pairs_gpt2(model_name, cultures=None, wave=7, excluding_topics=[],\n",
        "                                 excluding_cultures=[], model=None, tokenizer=None, use_cuda=False, prompt_mode='in'):\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "        tokenizer, model = get_gpt2_model(model_name, use_cuda)\n",
        "\n",
        "    wvs_df = get_wvs_df()\n",
        "\n",
        "    gpt2_all = []\n",
        "\n",
        "    for culture in tqdm(cultures):\n",
        "        if culture in excluding_cultures:\n",
        "            continue\n",
        "        prompts = wvs_gpt2_prompts_ratings_multiple_tokens(wvs_df, culture, prompt_mode)\n",
        "\n",
        "        culture_name = culture if culture else 'universal'\n",
        "        for question, rating_pairs in prompts.items():\n",
        "            if any(excluded_topic in question for excluded_topic in excluding_topics):\n",
        "                continue\n",
        "\n",
        "            lm_score, moral_log_probs, nonmoral_log_probs = get_log_prob_difference(rating_pairs, 0, 1, model, tokenizer, use_cuda)\n",
        "\n",
        "            wvs_score = rating_pairs[0][2]  #All token pairs have the same wvs_score\n",
        "\n",
        "            row = {\n",
        "                'country': culture_name, 'topic': question, 'wvs_score': wvs_score,\n",
        "                'moral log prob': moral_log_probs, 'non moral log probs': nonmoral_log_probs,\n",
        "                'log prob difference': lm_score\n",
        "            }\n",
        "\n",
        "            gpt2_all.append(row)\n",
        "\n",
        "    df = pd.DataFrame(gpt2_all)\n",
        "\n",
        "    survey_scores = df['wvs_score'].values\n",
        "    log_prob_diffs = df['log prob difference'].values\n",
        "\n",
        "    correlation, normalized_log_probs, p_value = calculate_correlation(survey_scores, log_prob_diffs)\n",
        "    if correlation is not None:\n",
        "      print(\"Correlation calculated successfully\")\n",
        "    else:\n",
        "      print(\"Failed to calculate correlation due to data length mismatch\")\n",
        "\n",
        "    df['normalized log prob difference'] = normalized_log_probs\n",
        "\n",
        "    metrics = {\n",
        "        'Pearson correlation coefficient': correlation,\n",
        "        'P value': p_value\n",
        "    }\n",
        "\n",
        "    return df, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "vD70Hree8Pyx"
      },
      "outputs": [],
      "source": [
        "def compare_pew_token_pairs_gpt2(cultures=None, wave=7, model_name='gpt2',\n",
        "                                 excluding_topics=[], excluding_cultures=[], model=None, tokenizer=None,\n",
        "                                 use_cuda=False, prompt_mode='in'):\n",
        "    if model is None or tokenizer is None:\n",
        "        tokenizer, model = get_gpt2_model(model_name, use_cuda)\n",
        "\n",
        "    pew_df = get_pew_df()\n",
        "\n",
        "    gpt2_all = []\n",
        "\n",
        "    for culture in tqdm(cultures):\n",
        "        if culture in excluding_cultures:\n",
        "            continue\n",
        "        prompts = pew_gpt2_prompts_ratings_multiple_tokens(pew_df, culture, prompt_mode)\n",
        "\n",
        "        culture_name = culture if culture else 'universal'\n",
        "        for question, rating_pairs in prompts.items():\n",
        "            if any(excluded_topic in question for excluded_topic in excluding_topics):\n",
        "                continue\n",
        "\n",
        "            lm_score, moral_log_probs, nonmoral_log_probs = get_log_prob_difference(rating_pairs, 0, 1, model, tokenizer, use_cuda)\n",
        "\n",
        "            pew_score = rating_pairs[0][2]  #All token pairs have the same pew_score\n",
        "\n",
        "            row = {\n",
        "                'country': culture_name, 'topic': question, 'pew_score': pew_score,\n",
        "                'moral log prob': moral_log_probs, 'non moral log probs': nonmoral_log_probs,\n",
        "                'log prob difference': lm_score\n",
        "            }\n",
        "\n",
        "            gpt2_all.append(row)\n",
        "\n",
        "    df = pd.DataFrame(gpt2_all)\n",
        "\n",
        "    survey_scores = df['pew_score'].values\n",
        "    log_prob_diffs = df['log prob difference'].values\n",
        "\n",
        "    correlation, normalized_log_probs, p_value = calculate_correlation(survey_scores, log_prob_diffs)\n",
        "    if correlation is not None:\n",
        "      print(\"Correlation calculated successfully\")\n",
        "    else:\n",
        "      print(\"Failed to calculate correlation due to data length mismatch\")\n",
        "\n",
        "    df['normalized log prob difference'] = normalized_log_probs\n",
        "\n",
        "    metrics = {\n",
        "        'Pearson correlation coefficient': correlation,\n",
        "        'P value': p_value\n",
        "    }\n",
        "\n",
        "    return df, metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsA_HV5L-UJy"
      },
      "source": [
        "# 1.Qwen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "N2cbQ8Q9-VqL"
      },
      "outputs": [],
      "source": [
        "TOKEN_PAIRS = [ ('always justifiable', 'never justifiable'),\n",
        "                ('right','wrong'),\n",
        "                ('morally good','morally bad'),\n",
        "                ('ethically right', 'ethically wrong'),\n",
        "                ('ethical','unethical')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "igXuu2sw-V0L"
      },
      "outputs": [],
      "source": [
        "#Function using Qwen model and WVS datatset\n",
        "\n",
        "df_Qwen_WVS, metrics_Qwen_WVS = compare_wvs_token_pairs(model_name=\"Qwen/Qwen2-0.5B\",\n",
        "                                       cultures = COUNTRIES_WVS_W7_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "output_file = 'results_Qwen_WVS.xlsx'\n",
        "df_Qwen_WVS.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {metrics_Qwen_WVS['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_Qwen_WVS['P value']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sPKSFm0V-V7q"
      },
      "outputs": [],
      "source": [
        "#Function using Qwen model and PEW datatset\n",
        "\n",
        "df_Qwen_PEW, metrics_Qwen_PEW = compare_pew_token_pairs(model_name=\"Qwen/Qwen2-0.5B\",\n",
        "                                       cultures = COUNTRIES_PEW_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "output_file = 'results_Qwen_PEW.xlsx'\n",
        "df_Qwen_PEW.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {metrics_Qwen_PEW['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_Qwen_PEW['P value']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HVGaDYK-zvS"
      },
      "source": [
        "# 2.BLOOM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "L1lTsy4i-sss"
      },
      "outputs": [],
      "source": [
        "TOKEN_PAIRS = [ ('always justifiable', 'never justifiable'),\n",
        "                ('right','wrong'),\n",
        "                ('morally good','morally bad'),\n",
        "                ('ethically right', 'ethically wrong'),\n",
        "                ('ethical','unethical')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl4m76c_-3oL",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#Function using BLOOM model and WVS datatset\n",
        "\n",
        "df_BLOOM_WVS, metrics_BLOOM_WVS = compare_wvs_token_pairs(model_name=\"bigscience/bloomz-560m\",\n",
        "                                       cultures = COUNTRIES_WVS_W7_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "output_file = 'results_BLOOM_WVS.xlsx'\n",
        "df_BLOOM_WVS.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {metrics_BLOOM_WVS['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_BLOOM_WVS['P value']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ew0P6mq7-3vq"
      },
      "outputs": [],
      "source": [
        "#Function using BLOOM model and PEW datatset\n",
        "\n",
        "df_BLOOM_PEW, metrics_BLOOM_PEW = compare_pew_token_pairs(model_name=\"bigscience/bloomz-560m\",\n",
        "                                       cultures = COUNTRIES_PEW_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "output_file = 'results_BLOOM_PEW.xlsx'\n",
        "df_BLOOM_PEW.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {metrics_BLOOM_PEW['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_BLOOM_PEW['P value']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DASLr3vB_NGo"
      },
      "source": [
        "# 3.OPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqqQ18t4_ftd"
      },
      "outputs": [],
      "source": [
        "TOKEN_PAIRS = [ ('always justifiable', 'never justifiable'),\n",
        "                ('right','wrong'),\n",
        "                ('morally good','morally bad'),\n",
        "                ('ethically right', 'ethically wrong'),\n",
        "                ('ethical','unethical')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WUfdJLfS_f1P"
      },
      "outputs": [],
      "source": [
        "#Function using OPT model and WVS datatset\n",
        "\n",
        "df_OPT_WVS, metrics_OPT_WVS = compare_wvs_token_pairs(model_name=\"facebook/opt-350m\",             #can also use \"facebook/opt-350m\"\n",
        "                                       cultures = COUNTRIES_WVS_W7_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "output_file = 'results_OPT_WVS.xlsx'\n",
        "df_OPT_WVS.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"\\n Pearson correlation coefficient: {metrics_OPT_WVS['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_OPT_WVS['P value']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2EGIlsAL_f8Z"
      },
      "outputs": [],
      "source": [
        "#Function using OPT model and PEW datatset\n",
        "\n",
        "df_OPT_PEW, metrics_OPT_PEW = compare_pew_token_pairs(model_name=\"facebook/opt-350m\",             #can also use \"facebook/opt-350m\"\n",
        "                                       cultures = COUNTRIES_PEW_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "output_file = 'results_OPT_PEW.xlsx'\n",
        "df_OPT_PEW.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"\\n Pearson correlation coefficient: {metrics_OPT_PEW['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_OPT_PEW['P value']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Nn7PV-EAxR6"
      },
      "source": [
        "# 4.GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CesZHeg3qD7"
      },
      "outputs": [],
      "source": [
        "TOKEN_PAIRS = [ ('always justifiable', 'never justifiable'),\n",
        "                ('right','wrong'),\n",
        "                ('morally good','morally bad'),\n",
        "                ('ethically right', 'ethically wrong'),\n",
        "                ('ethical','unethical')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HH2-2yMU95rc"
      },
      "outputs": [],
      "source": [
        "#Function using one of the GPT2 models and WVS datatset\n",
        "\n",
        "df_GPT2_WVS, metrics_GPT2_WVS = compare_wvs_token_pairs_gpt2(model_name=\"gpt2\",                   #can also use \"gpt2-medium\" or \"gpt2-large\"\n",
        "                                       cultures = COUNTRIES_WVS_W7_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "output_file = 'results_GPT2_base_WVS.xlsx'\n",
        "df_GPT2_WVS.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"\\n Pearson correlation coefficient: {metrics_GPT2_WVS['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_GPT2_WVS['P value']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MVRwHVOx-QNz"
      },
      "outputs": [],
      "source": [
        "#Function using one of the GPT2 models and PEW datatset\n",
        "\n",
        "df_GPT2_PEW, metrics_GPT2_PEW = compare_pew_token_pairs_gpt2(model_name=\"gpt2\",                        #can also use \"gpt2-medium\" or \"gpt2-large\"\n",
        "                                       cultures = COUNTRIES_PEW_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "output_file = 'results_GPT2_base_PEW.xlsx'\n",
        "df_GPT2_PEW.to_excel(output_file, index=False)\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {metrics_GPT2_PEW['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_GPT2_PEW['P value']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX1OYmjSbAqT"
      },
      "source": [
        "# (5)Gemma model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "1e59bf33987a4f34bb3445589d0169e5"
          ]
        },
        "id": "J7OAyHNP3NOQ",
        "outputId": "28618f16-6114-4e8d-d040-c7d75f030e3e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e59bf33987a4f34bb3445589d0169e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGmrmPSpezQo"
      },
      "outputs": [],
      "source": [
        "# TOKEN_PAIRS = [ ('always justifiable', 'never justifiable'),\n",
        "#                 ('right','wrong'),\n",
        "#                 ('morally good','morally bad'),\n",
        "#                 ('ethically right', 'ethically wrong'),\n",
        "#                 ('ethical','unethical')]\n",
        "\n",
        "TOKEN_PAIRS = [ ('always justifiable', 'never justifiable')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfNVbG8DX3xD"
      },
      "outputs": [],
      "source": [
        "#Function using Gemma model and WVS datatset\n",
        "\n",
        "df_Gemma_WVS, metrics_Gemma_WVS = compare_wvs_token_pairs(model_name=\"google/gemma-2b-it\",\n",
        "                                       cultures = COUNTRIES_WVS_W7_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "print(f\"\\n Pearson correlation coefficient: {metrics_Gemma_WVS['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_Gemma_WVS['P value']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l489K104YRZt"
      },
      "outputs": [],
      "source": [
        "#Function using Gemma model and WPEWVS datatset\n",
        "\n",
        "df_Gemma_PEW, metrics_Gemma_PEW = compare_pew_token_pairs(model_name=\"google/gemma-2b-it\",\n",
        "                                       cultures = COUNTRIES_PEW_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {metrics_Gemma_PEW['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_Gemma_PEW['P value']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URLHfVVGX4NB"
      },
      "source": [
        "# (6)LLaMA model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWUJsOkLbD70"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DFZ8MjEbeAn"
      },
      "outputs": [],
      "source": [
        "TOKEN_PAIRS = [ ('always justifiable', 'never justifiable'),\n",
        "                ('right','wrong'),\n",
        "                ('morally good','morally bad'),\n",
        "                ('ethically right', 'ethically wrong'),\n",
        "                ('ethical','unethical')]\n",
        "\n",
        "# TOKEN_PAIRS = [ ('always justifiable', 'never justifiable')]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vhzQIA8dNPK"
      },
      "outputs": [],
      "source": [
        "#Function using LLaMA model and WVS datatset\n",
        "\n",
        "df_LLaMa_WVS, metrics_LLaMa_WVS = compare_wvs_token_pairs(model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
        "                                       cultures = COUNTRIES_WVS_W7_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "print(f\"\\n Pearson correlation coefficient: {metrics_LLaMa_WVS['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_LLaMa_WVS['P value']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAJgjnrbe1TE"
      },
      "outputs": [],
      "source": [
        "#Function using LLaMA model and PEW datatset\n",
        "\n",
        "df_LLaMa_PEW, metrics_LLaMa_PEW = compare_pew_token_pairs(model_name=\"meta-llama/Meta-Llama-3-8B\",\n",
        "                                       cultures = COUNTRIES_PEW_ALL, use_cuda=False, prompt_mode='in') #can also use prompt_mode='people'\n",
        "\n",
        "print(f\"Pearson correlation coefficient: {metrics_LLaMa_PEW['Pearson correlation coefficient']}\")\n",
        "print(f\"P-value: {metrics_LLaMa_PEW['P value']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIa8y6iQe8sB"
      },
      "source": [
        "# Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "3XHnM_lJ7KKV"
      },
      "outputs": [],
      "source": [
        "#WVS datasets\n",
        "\n",
        "df = df_GPT2_WVS\n",
        "# df = df_OPT_WVS\n",
        "# df = df_Qwen_WVS\n",
        "# df = df_BLOOM_WVS\n",
        "\n",
        "#PEW datasets\n",
        "\n",
        "# df = df_GPT2_PEW\n",
        "# df = df_OPT_PEW\n",
        "# df = df_Qwen_PEW\n",
        "# df = df_BLOOM_PEW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3LcezNZF1VUt"
      },
      "outputs": [],
      "source": [
        "#0. Creating a boxplot to visualize the variations in moral scores between countries across topics in thw WVS dataset\n",
        "df = df_GPT2_WVS\n",
        "\n",
        "plt.figure(figsize=(7, 5))  #9,6\n",
        "sns.boxplot(data=df, x='wvs_score', y='topic', hue='topic', orient='h', palette=\"husl\", legend=False)\n",
        "\n",
        "plt.xlim(-1, 1)\n",
        "plt.title('Distribution of WVS Moral Scores by Topic',fontsize=10)\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dvogri155mLj"
      },
      "outputs": [],
      "source": [
        "#1.Creating a boxplot to visualize the variations in moral scores between countries across topics\n",
        "\n",
        "plt.figure(figsize=(7, 4))  #7, 5 WVS\n",
        "sns.boxplot(data=df, x='normalized log prob difference', y='topic', hue='topic', orient='h', palette=\"husl\", legend=False)\n",
        "\n",
        "plt.xlim(-1, 1)\n",
        "# plt.title('Moral scores inferred from Qwen model for WVS',fontsize=10)  #GPT2 base model  BLOOM-560M  OPT-125M\n",
        "plt.xlabel('')\n",
        "plt.ylabel('')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "roUMVnZxF1Yk",
        "e0YMLS_IF6yS",
        "K6-0jkmYGucL",
        "trRrPYkqHq82",
        "SClE4R6mmJLm",
        "NaeQRYO4kV2s",
        "poNF1tlMkElB",
        "jsvE58Gvi2Ev",
        "5xlVS8564zLO",
        "ZsA_HV5L-UJy",
        "4HVGaDYK-zvS",
        "DASLr3vB_NGo",
        "9Nn7PV-EAxR6",
        "rX1OYmjSbAqT",
        "URLHfVVGX4NB",
        "KIa8y6iQe8sB"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}